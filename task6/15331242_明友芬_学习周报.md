## 1、Nova简介

Nova 是 OpenStack 最核心的服务，负责维护和管理云环境的计算资源。所以我们小组选择深入学习nova组件。OpenStack 作为 IaaS 的云操作系统，虚拟机生命周期管理也就是通过 Nova 来实现的。
OpenStack中一台虚拟机建立的工作流程：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/yxf-develop/task6/image/5.png" width="60%" height="60%" /></div>

### Nova架构：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/6.png" width="60%" height="60%" /></div>


#### （1）API
- nova-api：接收和响应客户的 API 调用。每个 OpenStack 组件可能包含若干子服务，其中必定有一个 API 服务负责接收客户请求。 以 Nova 为例，nova-api 作为 Nova 组件对外的唯一窗口，向客户暴露 Nova 能够提供的功能。 当客户需要执行虚机相关的操作，能且只能向 nova-api 发送 REST 请求。 这里的客户包括终端用户、命令行和 OpenStack 其他组件。 

设计 API 前端服务的好处在于： 1. 对外提供统一接口，隐藏实现细节 2. API 提供 REST 标准调用服务，便于与第三方系统集成 3. 可以通过运行多个 API 服务实例轻松实现 API 的高可用，比如运行多个 nova-api 进程。
#### （2）compute core
- nova-scheduler：虚机调度服务，负责决定在哪个计算节点上运行虚机。对于某项操作，如果有多个实体都能够完成任务，那么通常会有一个 scheduler 负责从这些实体中挑选出一个最合适的来执行操作。除了 Nova，块服务组件 Cinder 也有 scheduler 子服务，后面我们会详细讨论
- nova-compute：管理虚机的核心服务，通过调用 Hypervisor API 实现虚机生命周期管理。
- Hypervisor（超级监督程序）：计算节点上跑的虚拟化管理程序，虚机管理最底层的程序。 不同虚拟化技术提供自己的 Hypervisor。 以 Nova 为例，OpenStack 的计算节点支持多种 Hypervisor。 包括 KVM, Hyper-V, VMWare, Xen, Docker, LXC 等。Nova-compute 为这些 Hypervisor 定义了统一的接口，hypervisor 只需要实现这些接口，就可以 driver 的形式即插即用到 OpenStack 中。 下面是 nova driver 的架构示意图。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/7.png" width="60%" height="60%" /></div>

- nova-conductor：nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor，这个我们在后面会详细讨论。 
#### （3）Console Interface
- nova-console：用户可以通过多种方式访问虚机的控制台： nova-novncproxy，基于 Web 浏览器的 VNC 访问 nova-spicehtml5proxy，基于 HTML5 浏览器的 SPICE 访问 nova-xvpnvncproxy，基于 Java 客户端的 VNC 访问。 
- nova-consoleauth：负责对访问虚机控制台请求提供 Token 认证。
- nova-cert：提供 x509 证书支持 
#### （4）Database
Nova 会有一些数据需要存放到数据库中，一般使用 MySQL。 数据库安装在控制节点上。 Nova 使用命名为 “nova” 的数据库。
#### （5）Message Queue
在前面我们了解到 Nova 包含众多的子服务，这些子服务之间需要相互协调和通信。 为解耦各个子服务，Nova 通过 Message Queue 作为子服务的信息中转站。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/8.png" width="60%" height="60%" /></div>

Scheduler 从 Messaging 接收到请求后执行调度操作，完成后将结果也通过 Messaging 发送给 API。
nova-compute 在计算节点上运行，负责管理节点上的 instance；nova-compute 与 Hypervisor 一起实现 OpenStack 对 instance 生命周期的管理；

## 2.nova-scheduler深入学习
### 2.1 nova-scheduler简介
本节重点介绍 nova-scheduler 的调度机制和实现方法：即解决如何选择在哪个计算节点上启动 instance 的问题。创建 Instance 时，用户会提出资源需求，例如 CPU、内存、磁盘各需要多少。OpenStack 将这些需求定义在 flavor 中，用户只需要指定用哪个 flavor 就可以了。（打开自己的dashboard截一个关于flavor的图）

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/9.png" width="60%" height="60%" /></div>

Flavor 主要定义了 VCPU，RAM，DISK 和 Metadata 这四类。 nova-scheduler 会按照 flavor 去选择合适的计算节点。
#### 2.2.1 nova-scheduler如何实现调度——过滤器（通过过滤器（filter）选择满足条件的计算节点）

##### （1）在/etc/nova/nova.conf中，nova通过以下三个参数的设置来实现调度：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/10.png" width="60%" height="60%" /></div>

##### （2）filter scheduler
Filter scheduler 是 nova-scheduler 默认的调度器，调度过程分为两步： 
- A、通过过滤器（filter）选择满足条件的计算节点（运行 nova-compute）
nova-compute 经常需要更新数据库，比如更新虚机的状态。 出于安全性和伸缩性的考虑，nova-compute 并不会直接访问数据库，而是将这个任务委托给 nova-conductor
经过 Filters 过滤后，会得到一个 Host 列表。这样的话 nova-scheduler 就需要从数据库中取得当前各个 Host 最新的资源使用情况，这些资源数据的收集和存储都由 nova-compute 中定义的数据库同步机制来完成。但是 nova-compute 对数据库的更新是周期性的， nova-scheduler 在选择最佳 Host 时需要最新的资源数据。所以在 nova-scheduler 中使用了 nova.scheduler.host_manager:HostState 来维护一份数据。这份数据仅保存在当前进程的内存中，里面包含了从上次数据库更新到现在 Host 资源的变化情况，也就是最新的 Host 资源数据。nova-scheduler 为了保持自己所维护的资源数据是最新的，每创建一个 Instance ，nova-scheduler 都要将这份资源数据更新，并从 Host 可用资源中去掉虚拟机使用的部分。 
注意：nova-scheduler 所维护的数据不会同步到数据库，它只会从数据库同步数据到自身，所以 nova-scheduler 并没有写数据库的功能
- B、通过权重计算（weighting）选择在最优（权重值最大）的计算节点上创建 Instance。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/11.png" width="60%" height="60%" /></div>

（官网上一个关于如何调度的图）
scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler。
Nova 允许使用第三方 scheduler，配置 scheduler_driver 即可。
##### （3）filter 
当 Filter scheduler 需要执行调度操作时，会让 filter 对计算节点进行判断，filter 返回 True 或 False。 

Nova.conf 中的 scheduler_available_filters 选项用于配置 scheduler 可用的 filter，默认是所有 nova 自带的 filter 都可以用于滤操作。 
scheduler_available_filters = nova.scheduler.filters.all_filters
另外还有一个选项 scheduler_default_filters，用于指定 scheduler 真正使用的 filter，默认值如下 

scheduler_default_filters = RetryFilter, AvailabilityZoneFilter, RamFilter, DiskFilter, ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter

Filter scheduler 将按照列表中的顺序依次过滤。 下面依次介绍每个 filter

### 2.2按调度顺序介绍主要几个调度器
- （1）RetryFilter
RetryFilter 的作用是刷掉之前已经调度过的节点。 举个例子方便大家理解： 假设 A,B,C 三个节点都通过了过滤，最终 A 因为权重值最大被选中执行操作。 但由于某个原因，操作在 A 上失败了。 默认情况下，nova-scheduler 会重新执行过滤操作（重复次数由 scheduler_max_attempts 选项指定，默认是 3）。 那么这时候 RetryFilter 就会将 A 直接刷掉，避免操作再次失败。 RetryFilter 通常作为第一个 filter。 
- （2）AvailabilityZoneFilter
为提高容灾性和提供隔离服务，可以将计算节点划分到不同的Availability Zone中。 
例如把一个机架上的机器划分在一个 Availability Zone 中。 OpenStack 默认有一个命名为 “Nova” 的 Availability Zone，所有的计算节点初始都是放在 “Nova” 中。 用户可以根据需要创建自己的 Availability Zone。 

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/12.png" width="60%" height="60%" /></div>

创建实例的时候，会让我们选择可用域：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/13.png" width="60%" height="60%" /></div>

nova-scheduler 在做 filtering 时，会使用 AvailabilityZoneFilter 将不属于指定 Availability Zone 的计算节点过滤掉

- （3）RamFilter
RamFilter 将不能满足 flavor 内存需求的计算节点过滤掉。 
对于内存有一点需要注意： 为了提高系统的资源使用率，OpenStack 在计算节点可用内存时允许 overcommit，也就是可以超过实际内存大小。 超过的程度是通过 nova.conf 中 ram_allocation_ratio 这个参数来控制的，默认值为 1.5 
ram_allocation_ratio = 1.5
查看电脑 /etc/nova/nova.conf:

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/14.png" width="60%" height="60%" /></div>

发现真实的比例是16.
其含义是：如果计算节点的内存有 10GB，OpenStack 则会认为它有 15GB（10*1.5）的内存
- （4）DiskFilter
DiskFilter 将不能满足 flavor 磁盘需求的计算节点过滤掉。 
Disk 同样允许 overcommit，通过 nova.conf 中 disk_allocation_ratio 控制，默认值为 1 
disk_allocation_ratio = 1.0
查看电脑 /etc/nova/nova.conf:

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/14.png" width="60%" height="60%" /></div>


- （5）CoreFilter
CoreFilter 将不能满足 flavor vCPU 需求的计算节点过滤掉。 
vCPU 同样允许 overcommit，通过 nova.conf 中 cpu_allocation_ratio 控制，默认值为 16。cpu_allocation_ratio = 16.0
查看电脑 /etc/nova/nova.conf:

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/14.png" width="60%" height="60%" /></div>

这意味着一个 8 vCPU 的计算节点，nova-scheduler 在调度时认为它有 128 个 vCPU。 需要提醒的是： nova-scheduler 默认使用的 filter 并没有包含 CoreFilter。 如果要用，可以将 CoreFilter 添加到 nova.conf 的 scheduler_default_filters 配置选项中。 
- （6）ComputeFilter
ComputeFilter 保证只有 nova-compute 服务正常工作的计算节点才能够被 nova-scheduler调度。ComputeFilter 显然是必选的 filter。
- （7）ComputeCapabilitiesFilter
ComputeCapabilitiesFilter 根据计算节点的特性来筛选。这个比较高级，我们举例说明。
例如我们的节点有 x86_64 和 ARM 架构的，如果想将 Instance 指定部署到 x86_64 架构的节点上，就可以利用到 ComputeCapabilitiesFilter。还记得 flavor 中有个 Metadata 吗，Compute 的 Capabilitie s就在 Metadata中 指定。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/15.png" width="60%" height="60%" /></div>

- （8）ImagePropertiesFilter
ImagePropertiesFilter 根据所选 image 的属性来筛选匹配的计算节点。跟 flavor 类似，image 也有 metadata，用于指定其属性。
- （9）ServerGroupAntiAffinityFilter
ServerGroupAntiAffinityFilter 可以尽量将 Instance 分散部署到不同的节点上。
例如有 inst1，inst2 和 inst3 三个 instance，计算节点有 A,B 和 C。
为保证分散部署，进行如下操作：
A、创建一个 anti-affinity 策略的 server group “group-1”
nova server-group-create --policy anti-affinity group-1
请注意，这里的 server group 其实是 instance group，并不是计算节点的 group。
依次创建 Instance，将inst1, inst2和inst3放到group-1中
nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst1
nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst2
nova boot --image IMAGE_ID --flavor 1 --hint group=group-1 inst3
因为 group-1 的策略是 AntiAffinity，调度时 ServerGroupAntiAffinityFilter 会将 inst1, inst2 和 inst3 部署到不同计算节点 A, B 和 C。
目前只能在 CLI 中指定 server group 来创建 instance。
- （10）ServerGroupAffinityFilter
与 ServerGroupAntiAffinityFilter 的作用相反，ServerGroupAffinityFilter 会尽量将 instance 部署到同一个计算节点上。
方法类似
创建一个 affinity 策略的 server group “group-2”
nova server-group-create --policy affinity group-2
依次创建 instance，将 inst1, inst2 和 inst3 放到 group-2 中
nova boot --image IMAGE_ID --flavor 1 --hint group=group-2 inst1
nova boot --image IMAGE_ID --flavor 1 --hint group=group-2 inst2
nova boot --image IMAGE_ID --flavor 1 --hint group=group-2 inst3
因为 group-2 的策略是 Affinity，调度时 ServerGroupAffinityFilter 会将 inst1, inst2 和 inst3 部署到同一个计算节点。
创建 instance 时如果没有指定 server group，ServerGroupAffinityFilter 会直接通过，不做任何过滤。
创建 instance 时如果没有指定 server group，ServerGroupAntiAffinityFilter 会直接通过，不做任何过滤
### 2.3 nova-scheduler如何实现调度——权重计算（通过权重计算（weighting）选择在最优（权重值最大）的计算节点上创建 Instance）

Scheduler 会对每个计算节点打分，得分最高的获胜。
打分的过程就是 weight，翻译过来就是计算权重值，那么 scheduler 是根据什么来计算权重值呢？
目前 nova-scheduler 的默认实现是根据计算节点空闲的内存量计算权重值：
空闲内存越多，权重越大，instance 将被部署到当前空闲内存最多的计算节点上


##  3.nova-scheduler源码理解

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/17.png" width="60%" height="60%" /></div>

### 3.1 阶段一：nova-scheduler 接收 build_instances RPC 远程调用
在openstack项目中，api的调用规则：
跨项目：如nova调用keystone， glance，cinder等，使用rest api（通过相应的python-XXXclient 库）；项目内跨服务调用，使用RPC调用，通过服务提供的rpcapi.py文件，比如cinder内部，cinder-api与cinder-volume，cinder-scheduler服务之间使用RPC接口，即RabbitMQ消息；
通过nova-conductor ==> RPC scheduler_client.seect_destination() ==> nova-scheduler
在nova-conductor的ComputeTaskManager类里面调用build_instance函数，

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/18.png" width="60%" height="60%" /></div>

为nova-conductor在计算任务的命名空间下产生一个rpc的API，这里主要是使用ComputeTaskManager类的build_instance方法，为要建立的instance实例计算出所有可用的资源，并产生一个Host列表。conductor/manager.py ComputeTaskManager的build_instances方法创建一个新的虚拟机实例时，使用scheduler的build_request_spec方法构造请求的资源规格，最终使用RPC调用select_destinations方法选择合适的节点。
Build_instance函数：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/19.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/20.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/21.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/22.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/23.png" width="60%" height="60%" /></div>

nova-conductor 在调用 nova-scheduler 来获取能够创建 Instance 的 Host 的同时也获取了：requested_networks/flavor 等信息。
其中获取 Hosts 列表的代码块：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/24.png" width="60%" height="60%" /></div>

这个函数调用如下所示：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/25.png" width="60%" height="60%" /></div>

这个函数首先调用了nova-scheduler里面的utils里的setup_instance_group函数

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/26.png" width="60%" height="60%" /></div>


然后调用客户端client里面的query里的select_destinations函数；这个函数有调用了rpcapi函数。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/27.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/28.png" width="60%" height="60%" /></div>


### 3.2 阶段二：从 scheduler.rpcapi.SchedulerAPI 到 scheduler.manager.SchedulerManager

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/29.png" width="60%" height="60%" /></div>

### 3.3 阶段三：从 scheduler.manager.SchedulerManager 到调度器 FilterScheduler
vim /etc/nova/nova.conf

scheduler_driver = nova.scheduler.filter_scheduler.FilterScheduler

从配置文件选项 scheduler_driver 的值可以知道，nova.scheduler.manager.SchedulerManager:driver 是 nova.scheduler.filter_scheduler.FilterScheduler 的实例化对象。 
所以跳转到 nova.scheduler.filter_scheduler.FilterScheduler:select_destinations() 。

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/30.png" width="60%" height="60%" /></div>

还有实现获取满足第一次过滤条件的主机列表的_schedule函数：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/31.png" width="60%" height="60%" /></div>

### 3.4 阶段四：从调度器FilterScheduler到过滤器Filters（如何实现所有的过滤工作的）
也即函数get_filtered_host()函数的具体实现

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/32.png" width="60%" height="60%" /></div>

继续跳转到get_filtered_objects处：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/33.png" width="60%" height="60%" /></div>

继续追踪查看filters的HostFilterHander()函数，这个函数实现了过滤器的基类：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/34.png" width="60%" height="60%" /></div>

在host_manager.py里面对过滤器的调度：

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/35.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/36.jpg" width="60%" height="60%" /></div>

### 3.5 阶段五：权重的计算函数

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/37.png" width="60%" height="60%" /></div>

追踪weight_handler:

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/38.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/39.png" width="60%" height="60%" /></div>

<div align=center> <img src="https://github.com/CourseCloudDesktop/cloudDesktop/tree/yxf-develop/task6/image/40.png" width="60%" height="60%" /></div>

## 4.实现总结感悟
本次实训对我来说最难，遇到问题最多的部分是前三个task，尤其是最开始部署OpenStack的时候出现了很多差错，也重新安装部署了很多次，总结一点经验教训如下：
- （1）在能力不足，对于OpenStack了解不够的时候不要强求自己使用官网文档，手动部署安装，最开始我们小组的设想是在一台机上部署计算节点，其余的机器上部署别的节点，结果看了官网文档，尝试了多次失败之后放弃了；
- （2）安装过程中失败了，要学会查看日志，根据解决对应的问题，不要一安装失败就暴力重装；
- （3）自己捣鼓不出来的时候多问问同学，不要一个人闷头瞎搞，事实上最开始部署OpenStack也是多亏了刘亚男同学的帮助才成功的；
- （4）多看官网文档，多看高质量的关于OpenStack的技术博客，这尽可能多的了解OpenStack在遇到问题的时候才能大致推测出问题所在，并尝试解决。
 本次的实训，学习了OpenStack的一些知识，虽然过程比较波折，但是整一个过程下来收获还是挺大的，感谢这个过程中帮助过我们小组的同学，也感谢这段时间TA姐姐的付出！
	




