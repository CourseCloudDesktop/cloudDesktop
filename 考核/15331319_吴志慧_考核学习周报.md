# 1. Neutorn架构：类似于其他服务，Neutron采用分布式架构，由多个组件共同对外提供网络服务。
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu1.png)

- Neutron Server 对外提供OpenStack网络API，接收请求，并调用 Plugin 处理请求。
- Plugin 处理 Neutron Server 发来的请求，维护 OpenStack 逻辑网络状态， 并调用 Agent 处理请求。
- Agent 处理 Plugin 的请求，负责在 network provider 上真正实现各种网络功能。
-	network provider 提供网络服务的虚拟或物理网络设备，例如 Linux Bridge，Open vSwitch 或者其他支持 Neutron 的物理交换机。
-	Queue Neutron Server，Plugin 和 Agent 之间通过 Messaging Queue 通信和调用。
-	Database 存放 OpenStack 的网络状态信息，包括 Network, Subnet, Port, Router 等。

# 2. 深入学习Neutron Server，plugin，agent。
## a. Neutron Server 分层结构：
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu2.png)

-	Core API
对外提供管理 network, subnet 和 port 的 RESTful API。
-	Extension API
对外提供管理 router, load balance, firewall 等资源 的 RESTful API。
-	Commnon Service
认证和校验 API 请求。
-	Neutron Core
Neutron server 的核心处理程序，通过调用相应的 Plugin 处理请求。
-	Core Plugin API
定义了 Core Plgin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Core Plgin。
-	Extension Plugin API
定义了 Service Plgin 的抽象功能集合，Neutron Core 通过该 API 调用相应的 Service Plgin。
-	Core Plugin
实现了 Core Plugin API，在数据库中维护 network, subnet 和 port 的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 network。
-	Service Plugin
实现了 Extension Plugin API，在数据库中维护 router, load balance, security group 等资源的状态，并负责调用相应的 agent 在 network provider 上执行相关操作，比如创建 router。
-	归纳起来，Neutron Server 包括两部分：
1. 提供 API 服务。
2. 运行 Plugin。

### Neutron server初始化
Neutron-Server启动，无外乎就是加载配置，router各种resource，然后就等待请求了。其中router哪些resource完全是由配置文件来决定的。配置文件指neutron代码的入口配置文件neutron.setup.cfg，我们可以通过这个文件了解整个项目的代码结构。
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu3.png)

-	neutron-l3-agent：l3 agent部署在计算节点或者网络节点上,负责3层虚拟网络的管理。根据setup.cfg文件可以看出neutron-l3-agent的代码路径是neutron\agent\l3\agent
-	neutron-openvswitch-agent：Open vSwitch Agent部署在计算节点或者网络节点上，进行管理OVS虚拟交换机。根据setup.cfg文件可以看出neutron-openvswitch-agent的代码路径是neutron\plugins\openvswitch\agent.ovs_neutron_agent
-	neutron-server:是Neutron中唯一的一个服务进程，负责接收用户的RESTful API请求并分发处理给各种agen来完成这些的任务。根据setup.cfg文件可以看出neutron代码路径是neutron\server
-	ML2Plugin：用于提供二层虚拟网络，实现了network/subnet/port资源的操作，这些操作最终由Plugin通过RPC调用OpenvSwitch Agent来完成。根据setup.cfg文件可以看出代码路径是 neutron\plugins\ml2\plugin\Ml2Plugin

#### server初始化
server初始化文件：neutron\neutron\server\__init__.py
main方法核心是调用serve_wsgi、serve_rpc，分别创建api服务和rpc服务。

![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu4.png)

## b.plugin与agent
-	plugin 解决的是 What 的问题，即网络要配置成什么样子？而至于如何配置 How 的工作则交由 agent 完成。
-	plugin，agent 和 network provider 是配套使用的，如果network provider 是 linux bridge，那么就得使用 linux bridge 的 plungin 和 agent；如果 network provider 换成了 OVS 或者物理交换机，plugin 和 agent 也得替换。
-	plugin 按照功能分为两类： core plugin 和 service plugin。core plugin 维护 Neutron 的 netowrk, subnet 和 port 相关资源的信息，与 core plugin 对应的 agent 包括 linux bridge, OVS 等； service plugin 提供 routing, firewall, load balance 等服务，也有相应的 agent。
### 1. plugin初始化
在server初始化时，serve_rpc函数会据配置文件core_plugin的配置加载plugin，然后创建RpcWorker，开始监听rpc；通过调用_plugin.start_rpc_listeners进行监听。以ml2 plugin为例，在它的start_rpc_listener方法中，创建neutron.plugin.ml2.rpc.RpcCallbacks类的实例。
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu5.png)

start_rpc_listener方法中的_setup_rpc函数，创建neutron.plugin.ml2.rpc.RpcCallbacks类的实例
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu6.png)

-	ML2Plugin初始化时，针对agent创建自己的消息队列（notify，生产者）（topics.AGENT）以便向agent发送rpc请求，同时订阅ml2 Agent消息队列的消息（consumer，消费者）（topics.PLUGIN）以便接收来自agent的rpc请求。
-	同样，ml2 Agent初始化时，也会创建自己的消息队列（notify，生产者）（topics.PLUGIN）来向plugin发送rpc请求，同时订阅ML2Plugin消息队列的消息（consumer，消费者）（topics.AGENT）来接收来自plugin的rpc请求。
-	消息队列的生成者类(xxxxNotifyAPI)和对应的消费者类（xxxxRpcCallback）定义有相同的接口函数，生产者类中的函数主要作用是rpc调用消费者类中的同名函数，消费者类中的函数执行实际的动作。

### 2.agent初始化
**以用的比较多的L2层ovs agent（openvswitch agent）为例**

从setup.cfg配置文件的以下内容可以知道，启动agent实际执行的方法是neutron.plugins.openvswitch.agent.ovs_neutron_agent:main。
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu7.png)

neutron/plugins/openvswitch/agent/ovs_neutron_agent.py:main方法源码如下：
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu8.png)

启动时做了以下工作： 
1. 设置plugin_rpc，这是用来与neutron-server通信的。 
2. 设置state_rpc，用于agent状态信息上报。 
3. 设置connection，用于接收neutron-server的消息。 
4. 启动状态周期上报。 
5. 设置br-int。 
6. 设置bridge_mapping对应的网桥。 
7. 初始化sg_agent，用于处理security group。 
8. 周期检测br-int上的端口变化，调用process_network_ports处理添加/删除端口。

其中OVSNeutronAgent的docstring中，说明了agent实现虚拟的方式
![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu9.png)

1) 创建br-int, br-tun以及每个物理网络接口一个bridge。 
2) 虚拟机的虚拟网卡都会接入到br-int。使用同一个虚拟网络的虚拟网卡共享一个local的VLAN（与外部网络的VLAN无关，vlan id可以重叠）。这个local的VLAN id会映射到外部网络的某个VLAN id。 
3) 对于network_type是VLAN或者FLAT的网络，在br-int和各个物理网络bridge之间创建一个虚拟网卡，用于限定流规则、映射或者删除VLAN id等处理。 
4) 对于network_type是GRE的，每个租户在不同hypervisor之间的网络通信通过一个逻辑交换机标识符（Logical Switch identifier)进行区分，并创建一个连通各个hypervisor的br-tun的通道(tunnel)网络。Port patching用于连通br-int和各个hypervisor的br-tun上的VLAN。

# 3. 了解linux bridge实现neutron网络的方法和过程，并描述简单实例。
Linux bridge实现neutron网络

![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu10.png)


linux bridge plugin
1.	与 neutron server 一起运行。（一般是控制节点上）
2.	实现了 core plugin API。
3.	负责维护数据库信息。
4.	通知 linux bridge agent 实现具体的网络功能。

linux bridge agent
1.	在计算节点和网络节点（或控制节点）上运行。
2.	接收来自 plugin 的请求。
3.	通过配置本节点上的 linux bridge 实现 neutron 网络功能。

**例子**
以创建一个 VLAN100 的 network 为例，假设 network provider 是 linux bridge， 流程如下：
1.	Neutron Server 接收到创建 network 的请求，通过 Message Queue（RabbitMQ）通知已注册的 Linux Bridge Plugin。
2.	Plugin 将要创建的 network 的信息（例如名称、VLAN ID等）保存到数据库中，并通过 Message Queue 通知运行在各节点上的 Agent。
3.	Agent 收到消息后会在节点上的物理网卡（比如 eth2）上创建 VLAN 设备（比如 eth2.100），并创建 bridge （比如 brqXXX） 桥接 VLAN 设备。即建立network provider。

**其中第三步，建立基于Linux bridge的VLAN类型的network provider过程：**

Linux Bridge 是 Linux 上用来做 TCP/IP 二层协议交换的设备，其功能可以简单的理解为是一个二层交换机。

![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu11.png)

-	eth0 是宿主机上的物理网卡，有一个命名为 eth0.10 的子设备与之相连。 eth0.10 就是 VLAN 设备了，其 VLAN ID 就是 VLAN 10。 eth0.10 挂在命名为 brvlan10 的 Linux Bridge 上，虚机 VM1 的虚拟网卡 vent0 也挂在 brvlan10 上。
-	这样的配置其效果就是： 宿主机用软件实现了一个虚拟交换机，上面定义了一个 VLAN10。 eth0.10，brvlan10 和 vnet0 都分别接到 VLAN10 的 Access口上。而 eth0 就是一个 Trunk 口。VM1 通过 vnet0 发出来的数据包会被打上 VLAN10 的标签。
-	eth0.10 的作用是：定义了 VLAN10
-	brvlan10 的作用是：Bridge 上的其他网络设备自动加入到 VLAN10 中

**open vswitch实现neutron网络与linux bridge实现neutron网络的方法类似，只需将linux bridge plugin和linux bridge agent替换成open vswitch plugin 和 open vswitch agent即可**

![](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/%E8%80%83%E6%A0%B8/images/wu12.png)



