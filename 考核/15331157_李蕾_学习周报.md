# 最终考核学习周报
周报内容根据学习过程记录  
## 一、理解neutron的作用  
### 回顾手动部署过程  
在实训最开始的时候我尝试过根据openstack的官方文档部署，回顾文档的部署步骤，在networking服务中，下图是网络服务概览的内容：  
![contents](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/content.PNG)  
其中介绍了neutron包含组件有：neutron-server、openstack网络插件和代理以及消息队列。其中消息队列用于在neutron-server和各种代理间的消息路由，也可用于对于特定插件作为一个数据库存储网络状态。  
官网上对于neutron的概念解释是：Openstack网络(neutron)管理OpenStack环境中所有虚拟网络基础设施(VNI)，物理网络基础设施(PNI)的接入层。在我自己的理解中，因为在Openstack中neutron是一个网络服务，也就是在opensack中为其他的服务提供网络连接服务，提供网络、子网以及路由这些对象供服务器和实例使用。  
下图是官网中安装neutron的过程，我整理了一下步骤，分为控制节点和计算节点的安装和配置：  
![steps](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/steps.PNG)  
可以看到，首先要做的是创建一个数据库，因为网络的配置信息需要存储，所以就需要数据库，这里就使用mysql创建了neutron数据库，之后创建neutron的服务实体以及网络服务的API端点。前置条件满足后，就要安装组件，前面提到的neutron的所有组件就在这一步得到了安装。各种组件在安装完成后进行配置，再重启服务，neutron服务就基本完成了。官网上的安装教程只是给安装的过程提供指引，并没有明确的说明各个组件是干吗的以及它们之间的联系，不过在安装的过程中了解了neutron的组件有哪些。  
### neutron的概念理解  
Neutron的设计目标是实现网络即服务，基于“软件定义网络”实现网络虚拟化。那么可以理解为neutron是用来创建虚拟网络的，当虚拟机启动的时候会有一个虚拟网卡，虚拟网卡会连接到虚拟的switch上，虚拟的switch连接到虚拟的router上，虚拟的router最终和物理网卡联通，从而虚拟网络和物理网络联通起来，neutron就是实现上述操作的网络服务。

## 二、学习neutron的组件 
### 各组件之间的关系：  
下图显示了Neutron内部各组件之间的关系：  
![neutron](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/neutron.PNG)  
Neutron Server：这一部分包含守护进程neutron-server和各种插件neutron-*-plugin，它们既可以安装在控制节点也可以安装在网络节点。neutron-server提供API接口，并把对API的调用请求传给已经配置好的插件进行后续处理。  
插件：访问数据库来维护各种配置数据和对应关系，例如路由器、网络、子网、端口、浮动IP、安全组等等。  
插件代理：虚拟网络上的数据包的处理由这些插件代理来完成的。在每个计算节点和网络节点上运行。一般来说你选择了什么插件，就需要选择相应的代理。  
消息队列：代理与Neutron Server及其插件的交互要通过消息队列来支持。  
3层代理： 名字为neutron-l3-agent， 为客户机访问外部网络提供3层转发服务。也部署在网络节点上。  
### 组件的部署  
下图显示了neutron在三节点openstack上各个组件的部署：  
![nodes](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/nodes.PNG)  
分三个网络：  
External Network/API Network，这个网络是连接外网的，无论是用户调用Openstack的API，还是创建出来的虚拟机要访问外网，或者外网要ssh到虚拟机，都需要通过这个网络。   
Data Network，数据网络，虚拟机之间的数据传输通过这个网络来进行，比如一个虚拟机要连接另一个虚拟机，虚拟机要连接虚拟的路由都是通过这个网络来进行。   
Management Network，管理网络，Openstack各个模块之间的交互，连接数据库，连接Message Queue都是通过这个网络来。

neutron分成多个模块分布在三个节点上。
Controller节点：
neutron-server，用于接受API请求创建网络，子网，路由器等，然而创建的这些东西仅仅是一些数据结构在数据库里面。  
Network节点：
neutron-l3-agent，用于创建和管理虚拟路由器，当neutron-server将路由器的数据结构创建好，它是做具体的事情的，真正的调用命令行将虚拟路由器，路由表，namespace，iptables规则全部创建好。   
neutron-dhcp-agent，用于创建和管理虚拟DHCP Server，每个虚拟网络都会有一个DHCP Server，这个DHCP Server为这个虚拟网络里面的虚拟机提供IP。   
neutron-openvswith-plugin-agent，这个是用于创建虚拟的L2的switch的，在Network节点上，Router和DHCP Server都会连接到二层的switch上。  
Compute节点：
neutron-openvswith-plugin-agent，这个是用于创建虚拟的L2的switch的，在Compute节点上，虚拟机的网卡也是连接到二层的switch上。  
### neutron server学习  
下图显示了neutron server的结构：  
![server](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/server.PNG)  
图中Quantum-common和Quantum Plugin部分，其实就是Neutron Server。Server包含了两个很重要的部分：API和plugin。  
API又分为两个部分，API Core和API Extensions。API Core可以看做是插件功能的最小集合，即每个插件都必须有的功能，也就是对网络、子网和端口的查询、加删和更新操作等。API Extensions一般是针对具体插件实现的，这样租户就可以利用这些插件独特的功能，比方说访问控制（ACL)和QoS。  
根据上面已经学习到的知识，按我自己的理解，我认为neutron server就是提供关于网络的API，当它接受了创建网络，子网，路由器等API请求后，把对API的调用请求传给插件进行处理，插件再访问数据库来维护网络配置数据。当然neutron server创建的都是数据库里的数据，具体的操作都需要agent再进行处理。  

## 三、了解neutron源码  
学习neutron源码neutron server创建网络部分  
可以直接从github上下载openstack的源码进行分析  
在neutron/plugins/ml2/plugin.py中函数Ml2Plugin.create_network()创建网络，neutron-server只是对数据库操作。  
Ml2Plugin.create_network()函数如下：  
![1](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.PNG)  
调用create_network_db()，创建network数据库对象：  
![1.1](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.1.PNG)  
调用type_manager.create_network_segments()：  
![1.4_2](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.4_2.PNG)  
-  _process_provider_create() 验证创建的参数（network_type, physical_network以及segmentation_id）,其中根据network_type来获取type_driver并调用type_driver的validate_provider_segment() 验证传入的segmentation_id是否可用。  
- 如果传了segmentation_id 则会调用type_driver的reserve_provider_segment()，  没传则创建一个segment，调用type_driver的allocate_tenant_segment()，创建一条allocation记录， 比如network_type为vlan则在ml2_vlan_allocations中创建一条记录。  
- plugins/ml2/db.py. add_network_segment() 创建segment的分配记录  

调用_process_l3_create()判断是否创建的是external network， 如果是会在externalnetworks表上创建一条记录。  
![1.3](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.3.PNG)  
调用type_manager.extend_network_dict_provider() 将create_network_segment中处理的值赋值给创建的network对象  
![1.5](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.5.PNG)  
调用mechainsm_manager的create_network_precommit()  
![1.6](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/1.6.PNG)  
前一步正常退出，则会commit数据库的create操作。  
从源码也可以了解到，neutron-server创建网络的过程并没有实际的操作，只是数据库的操作。  
## 四、了解open vswitch实现neutron网络  
学习OVS+VLAN组网，经过学习后我认为ovs在这里实现了虚拟交换机的作用。  
在一个计算节点上的网络实例，neutron使用ovs  
![ovs](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/ovs.PNG)  
Neutron在该计算节点上:
- 创建了OVS Integration bridge br-int。
- 创建了一对patch port，连接 br-int和br-eth1。 
- 设置br-int中的flow rules。对虚机过来的从 access ports进入br-int的数据帧，会被加上相应的 VLAN Tag，转发到patch port；从patch port进入的数据帧，将VLAN ID 101修改为1, 102修改为2，再转发到相应的Access ports。  
- 设置br-eth1中的flow rules。从patch port进入的数据帧，将内部VLAN ID 1修改为101，内部VLAN ID 2修改为102，再从eth1端口发出。对从eth1进入的数据帧做相反的处理。 

可以观察同一台物理服务器上的网络流向：  
![flow](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/flow.PNG)  
如果VM1和VM2属于同一个tenant network的同一个subnet，那么两者的通信直接经过 br-int 进行。此时可以类比两台物理机连接在一个交换机上，如果这两个物理机处于同一子网，那么可以通过交换机直接通信。  
![flow2](https://github.com/CourseCloudDesktop/cloudDesktop/blob/wcl-develop/考核/images/flow2.PNG)  
如上图，如果两个虚拟机并不处于同一子网，就算两台虚拟机是处在同一个物理服务器上，仍然都算作跨子网的数据流向，需要经过网络节点中的Router进行IP包的路由。不能从综合网桥处直接通信，这里也可以看到ovs实现了交换机的功能。处于不同子网的物理机就算连接在同一交换机上，数据传递也需要经过路由器才能通信。  

