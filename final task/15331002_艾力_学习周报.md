## 学习周报

### Neutron 组件：
- Nova简介

nova和swift是openstack最早的两个组件，nova分为控制节点和计算节点，
计算节点通过nova computer进行虚拟机创建，通过libvirt调用kvm创建虚拟机，nova之间通信通过rabbitMQ队列进行通信。
 



<img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/kek-develop/final%20task/pic/AliPic/Nova%20%E6%9E%B6%E6%9E%84.png" width="60%">




- Nova 体系架构
 
<img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/kek-develop/final%20task/pic/AliPic/Nova%20%E4%BD%93%E7%B3%BB.png" width="60%">

### Nova重要组件介绍:


· 我主要负责Nova 组件中的 scheduler 模块。</br>

"Nova-Scheduler主要完成虚拟机实例的调度分配任务，创建虚拟机时，虚拟机该调度到哪台物理机上，迁移时若没有指定主机，
也需要经过scheduler。资源调度是云平台中的一个很关键问题，如何做到资源的有效分配，如何满足不同情况的分配方式，这些都需要nova-scheduler来掌控，
并且能够很方便的扩展更多的调度方法，可能需要虚拟机调度到空闲的机器，可能还需要将某类型的虚拟机调度到固定的机架等等"</br>


nova scheduler模块在openstack中的作用是决策虚拟机创建在哪个主机（计算节点）上。决策一个虚拟机应该调度到某物理节点，需要分为两个步骤：</br>
-	过滤（filter），过滤出可以创建虚拟机的主机
 
 <img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/kek-develop/final%20task/pic/AliPic/Filter%20scheduler.png" width="60%">
 

- 计算权值或称重（weight），根据权重大小进行分配，默认根据资源可用空间进行权重排序</br>

称重weight，通过HostManager的get_weighed_hosts实现, 为什么要称重呢？我们可能有多个主机通过过滤filter，
那我们怎么决定实例落在哪个主机上了，这就是称重的作用，帮助我们对这些主机进行排序, 以选取出相对最优的主机。</br>

<img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/kek-develop/final%20task/pic/AliPic/weight.png" width="60%">
 
NOTE：
除了nova-conductor可以访问数据库之外，因为nova-scheduler是只读数据库，而nova-api对数据库的操作有Policy保护，所以它们也都是可以访问数据库的。
但是最好还是仅通过nova-conductor来访问数据库。</br>

### 具体的调度过程：
-	Scheduler 启动分析</br>
1）	compute-api通过一层层调用，最后调用/nova/compute/manager.py的run_instance（）,如下
``` python 
	#启动虚拟机
	def run_instance(self, context, request_spec, admin_password,
            injected_files, requested_networks, is_first_time,
            filter_properties, legacy_bdm_in_spec=True):
        """Tries to call schedule_run_instance on the driver.
	
        Sets instance vm_state to ERROR on exceptions
        """
        instance_uuids = request_spec['instance_uuids']
		"""Compute-related Utilities and helpers."""
		#compute_utils = utils
		#EventReoprt()是一个类
        with compute_utils.EventReporter(context, conductor_api.LocalAPI(),
                                         'schedule', *instance_uuids):
            try:
				 
				#调用driver（filter_scheduler.py）中的schedule_run_instance（）
                return self.driver.schedule_run_instance(context,
                        request_spec, admin_password, injected_files,
                        requested_networks, is_first_time, filter_properties,
                        legacy_bdm_in_spec)
```
最后调用self.driver.schedule_run_instance(context,***）
我们再来看看，这个driver是什么?
``` python
scheduler_driver_opt = cfg.StrOpt('scheduler_driver',
        default='nova.scheduler.filter_scheduler.FilterScheduler',
        help='Default driver to use for the scheduler')
``` 
这里配置文件中，Scheduler_driver的默认值default = nova.scheduler.filter_scheduler.FilterScheduler
注意，在早些版本的时候，这个默认值都是MultiScheduler，现在nova-volumn用一个新的项目cinder代替了，所以，也就不采用原来的那个方案了。直接driver= FilterScheduler

  

2）通过self.driver.schedule_run_instance(context,***）调用，我们转到了/nova/scheduler/filter_scheduler.py文件，看到class FilterScheduler类。
``` python
def schedule_run_instance(self, context, request_spec,
                              admin_password, injected_files,
                              requested_networks, is_first_time,
                              filter_properties, legacy_bdm_in_spec):
        """This method is called from nova.compute.api to provision
        an instance.  We first create a build plan (a list of WeightedHosts)
        and then provision.
        Returns a list of the instances created.
        """
        payload = dict(request_spec=request_spec)
        self.notifier.info(context, 'scheduler.run_instance.start', payload)
 
        instance_uuids = request_spec.get('instance_uuids')
        LOG.info(_("Attempting to build %(num_instances)d instance(s) "
                    "uuids: %(instance_uuids)s"),
                  {'num_instances': len(instance_uuids),
                   'instance_uuids': instance_uuids})
        LOG.debug(_("Request Spec: %s") % request_spec)
```
schedule_run_instance（）这个函数里面有一个非常重要的语句是：
``` python 
#*******************开始调用_schedule方法，来实现调度******************************#
		#关键调用
		#通过下面的_schedule()进行调度，返回list_host
        weighed_hosts = self._schedule(context, request_spec,
                                       filter_properties, instance_uuids)
#**********************************************************************************#
```
上述通过_scheduler来调度产生最终筛选好的compute node，即weight_hosts(当然，它返回的是一个列表)</br>
- 让我们看看_scheduler()到底做了什么？</br>
         _scheduler()方法的标准定义如下：</br>
                    ""Returns a list of hosts that meet the required specs,ordered by their fitness.""  
                    
FilterScheduler的过程分为两步=Filter+weight，如下图所示
 <img src="https://github.com/CourseCloudDesktop/cloudDesktop/blob/kek-develop/final%20task/pic/AliPic/filter.jpg" width="60%">
 
这一个图是官方提供的非常经典的主机过滤过程流程图，照着这个图，我们再去理解代码的含义会相对简单一点。
过程分为1->2->3，最后黄色的host即为_schedule的返回值，下面是对该图过程的具体解释


（1）	先解决途中1的hosts是从哪里来的，在_schedule中，我们看到了如下的调用：
hosts = self.host_manager.get_all_host_states(elevated)
再来看上面的函数，我们打开/nova/shceduler/host_manager.py文件，找到对应的get_all_states()方法
``` python
def get_all_host_states(self, context):
        """Returns a list of HostStates that represents all the hosts
        the HostManager knows about.
		Also, each of the consumable resources
        in HostState are pre-populated and adjusted based on data in the db.
        """
 
        # Get resource usage across the available compute nodes:
		#通过下面的语句，在数据库中得到所有的计算节点信息
		#IMPL.compute_node_get_all(context, no_date_fields)
        compute_nodes = db.compute_node_get_all(context)
```
（2）	在获取到了hosts列表之后，接下来的工作当然是图中的第2步，Filter，函数调用如下：
``` python
hosts = self.host_manager.get_filtered_hosts(hosts,
                    filter_properties, index=num)
```
将filter_proprerties，hosts作为参数传到get_filtered_hosts()函数中去，得到Filter之后的hosts

（3）	第一次筛选过后，要开始计算每个host的权值，也就是图中的第三步。
``` python
weighed_hosts = self.host_manager.get_weighed_hosts(hosts,
                    filter_properties)
```
（4）最后返回选择好的主机给compute去执行

在_schedule函数中：
``` pyhon
return selected_hosts
```
在/nova/scheduler/filetr_scheduler中，将weight_hosts再通过另外的函数去执行。
``` python 
weighed_hosts = self._schedule(context, request_spec,
                                       filter_properties, instance_uuids)

```

## Nova scheduler 代码详解：

```python
当在conductor中通过_schedule_instances 来调用select_destinations
 def _schedule_instances(self, context, request_spec, filter_properties):
        scheduler_utils.setup_instance_group(context, request_spec,
                                             filter_properties)
        # TODO(sbauza): Hydrate here the object until we modify the
        # scheduler.utils methods to directly use the RequestSpec object
        spec_obj = objects.RequestSpec.from_primitives(
            context, request_spec, filter_properties)
        hosts = self.scheduler_client.select_destinations(context, spec_obj)
        return hosts

从computerTaskmanager中__init__ 中的赋值
from nova.scheduler import client as scheduler_client
self.scheduler_client = scheduler_client.SchedulerClient()
可以看出调用select_destinations 是会调用到scheduler/query.py 中的
from nova.scheduler import rpcapi as scheduler_rpcapi
class SchedulerQueryClient(object):
    """Client class for querying to the scheduler."""

    def __init__(self):
        self.scheduler_rpcapi = scheduler_rpcapi.SchedulerAPI()
    def select_destinations(self, context, spec_obj):
        """Returns destinations(s) best suited for this request_spec and
        filter_properties.

        The result should be a list of dicts with 'host', 'nodename' and
        'limits' as keys.
        """
        return self.scheduler_rpcapi.select_destinations(context, spec_obj)
可以看出scheduler_rpcapi 就是nova.scheduler.rpcapi.py 
从SchedulerAPI中的
    def select_destinations(self, ctxt, spec_obj):
        version = '4.3'
        msg_args = {'spec_obj': spec_obj}
        if not self.client.can_send_version(version):
            del msg_args['spec_obj']
            msg_args['request_spec'] = spec_obj.to_legacy_request_spec_dict()
            msg_args['filter_properties'
                     ] = spec_obj.to_legacy_filter_properties_dict()
            version = '4.0'
        cctxt = self.client.prepare(version=version)
        return cctxt.call(ctxt, 'select_destinations', **msg_args)
可以看出这里通过cctxt.call 来进行rpc远程调用，cctxt.call表示会阻塞进程，等待执行结果的返回.这里远程调用到scheduler/manager.py
    def select_destinations(self, ctxt,
                            request_spec=None, filter_properties=None,
                            spec_obj=_sentinel):
        """Returns destinations(s) best suited for this RequestSpec.

        The result should be a list of dicts with 'host', 'nodename' and
        'limits' as keys.
        """

        # TODO(sbauza): Change the method signature to only accept a spec_obj
        # argument once API v5 is provided.
        if spec_obj is self._sentinel:
            spec_obj = objects.RequestSpec.from_primitives(ctxt,
                                                           request_spec,
                                                           filter_properties)
        dests = self.driver.select_destinations(ctxt, spec_obj)
        return jsonutils.to_primitive(dests)

这里会调用driver.select_destinations，这里的driver 会从nova.conf文件中读取
    def __init__(self, scheduler_driver=None, *args, **kwargs):
        if not scheduler_driver:
            scheduler_driver = CONF.scheduler_driver
        try:
            self.driver = driver.DriverManager(
                    "nova.scheduler.driver",
                    scheduler_driver,
                    invoke_on_load=True).driver
在nova.conf 中一般赋值如下：
scheduler_driver = nova.scheduler.filter_scheduler.FilterScheduler

这里的调度器主要有三个实现分别是filter_scheduler.py/chance.py/caching_scheduler.py。从nova.conf中给
scheduler_driver 赋值来看，这三个调度器不能共存，必须选择一个，这里默认用的是FilterScheduler
其中FilterScheduler(driver.Scheduler)，也就是说driver.Scheduler是FilterScheduler的父类
而driver.Scheduler 实现如下：
class Scheduler(object):
    """The base class that all Scheduler classes should inherit from."""

    def __init__(self):
        self.host_manager = driver.DriverManager(
                "nova.scheduler.host_manager",
                CONF.scheduler_host_manager,
                invoke_on_load=True).driver
        self.servicegroup_api = servicegroup.API()

    def run_periodic_tasks(self, context):
        """Manager calls this so drivers can perform periodic tasks."""
        pass

    def hosts_up(self, context, topic):
        """Return the list of hosts that have a running service for topic."""

        services = objects.ServiceList.get_by_topic(context, topic)
        return [service.host
                for service in services
                if self.servicegroup_api.service_is_up(service)]

    @abc.abstractmethod
    def select_destinations(self, context, spec_obj):
        """Must override select_destinations method.

        :return: A list of dicts with 'host', 'nodename' and 'limits' as keys
            that satisfies the request_spec and filter_properties.
        """
        return []
可见实现了run_periodic_tasks/hosts_up/select_destinations 这三个接口.子类只要实现这三个接口中的至少一个就行，这里最重要的是select_destinations，也就是要自己写调度器的话，必须实现这个接口.
其中CachingScheduler(filter_scheduler.FilterScheduler):的实现比较简单，仅仅是调用FilterScheduler的
select_destinations

我们这里以chance.py 为例
 def _schedule(self, context, topic, spec_obj):
        """Picks a host that is up at random."""

        elevated = context.elevated()
        hosts = self.hosts_up(elevated, topic)
        if not hosts:
            msg = _("Is the appropriate service running?")
            raise exception.NoValidHost(reason=msg)

        hosts = self._filter_hosts(hosts, spec_obj)
        if not hosts:
            msg = _("Could not find another compute")
            raise exception.NoValidHost(reason=msg)

        return random.choice(hosts)
    def select_destinations(self, context, spec_obj):
        """Selects random destinations."""
        num_instances = spec_obj.num_instances
        # NOTE(timello): Returns a list of dicts with 'host', 'nodename' and
        # 'limits' as keys for compatibility with filter_scheduler.
        dests = []
        for i in range(num_instances):
            host = self._schedule(context, CONF.compute_topic, spec_obj)
            host_state = dict(host=host, nodename=None, limits=None)
            dests.append(host_state)

        if len(dests) < num_instances:
            reason = _('There are not enough hosts available.')
            raise exception.NoValidHost(reason=reason)
        return dests

在select_destinations 通过调用_schedule 来在符合条件的host中随机选择一个host。最终要的一条语句就是
random.choice(hosts)，这也是chance.py 这个调度器命名的原因.
```


